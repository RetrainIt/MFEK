{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a668088d-4036-4928-bdf9-69f55643ab52",
   "metadata": {},
   "source": [
    "# Extract Knowledge from ChatGPT (Azure for example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "243200c5-3a14-485e-9a33-fcb182e13187",
   "metadata": {},
   "source": [
    "### Package Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4732052-54ad-4285-819c-8594aad652b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai\n",
    "!pip install -q salesforce-lavis\n",
    "!pip install -q easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38d830cc-1587-4ccd-b2b9-5c00fc59f9c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c61f37e8-1479-41e0-a81d-f296b35b23f9",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d7d6d54-3b38-45a7-b299-8a4d8984ea1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data=pd.read_pickle('/data/twitter.pkl')\n",
    "data = pd.DataFrame() #given a sample here\n",
    "event = [' 2020 Summer Olympics','2014 Hong Kong protests']\n",
    "image = ['/data/dataset/datakey/images/2014 Hong Kong protests/94073.jpg','/data/dataset/datakey/images/2020 Summer Olympics/29365.jpg']\n",
    "text = ['Only this side of Lennon Wall left #LennonWallHK',\"Jeev Milkha Singh emotionally thanks Neeraj Chopra for fulfilling his Father's wish of getting a #Gold in #Athetics at the üôè #Tokyo2020 #Cheer4India #TeamIndia #IND\"]\n",
    "data['event'] = event\n",
    "data['image'] = image\n",
    "data['text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b1ea557-f40e-408c-964f-9a51346d46a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event</th>\n",
       "      <th>image</th>\n",
       "      <th>text</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020 Summer Olympics</td>\n",
       "      <td>/data/dataset/datakey/images/2014 Hong Kong pr...</td>\n",
       "      <td>Only this side of Lennon Wall left #LennonWallHK</td>\n",
       "      <td>a man and a woman standing in front of a wall ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014 Hong Kong protests</td>\n",
       "      <td>/data/dataset/datakey/images/2020 Summer Olymp...</td>\n",
       "      <td>Jeev Milkha Singh emotionally thanks Neeraj Ch...</td>\n",
       "      <td>a man sitting on top of a bench holding a medal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     event                                              image  \\\n",
       "0     2020 Summer Olympics  /data/dataset/datakey/images/2014 Hong Kong pr...   \n",
       "1  2014 Hong Kong protests  /data/dataset/datakey/images/2020 Summer Olymp...   \n",
       "\n",
       "                                                text  \\\n",
       "0   Only this side of Lennon Wall left #LennonWallHK   \n",
       "1  Jeev Milkha Singh emotionally thanks Neeraj Ch...   \n",
       "\n",
       "                                             caption  \n",
       "0  a man and a woman standing in front of a wall ...  \n",
       "1    a man sitting on top of a bench holding a medal  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a971fcca-24e0-47ba-a621-9f613ce69df5",
   "metadata": {},
   "source": [
    "### Extract Caption from Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610e22ef-c672-4dbd-9c77-5cf033c0f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavis.models import load_model_and_preprocess\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791aaca5-55e2-4128-8539-faf139db6015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup device to use\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, vis_processors, _ = load_model_and_preprocess(name=\"blip_caption\", model_type=\"base_coco\", is_eval=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98e2cafc-3b80-40d2-ad22-8f44eeff6f25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5 #Using batch\n",
    "caption_list=[]\n",
    "def generate_captions(model, vis_processors, images_path_list) -> torch.Tensor():\n",
    "\n",
    "    global BATCH_SIZE\n",
    "    pbar = tqdm(total = len(images_path_list)+1)\n",
    "    while len(images_path_list) != 0:\n",
    "        if len(images_path_list) > BATCH_SIZE:\n",
    "            tmp_img_path_list = images_path_list[:BATCH_SIZE]\n",
    "            images_path_list = images_path_list[BATCH_SIZE:]\n",
    "            pbar.update(BATCH_SIZE)\n",
    "        else:\n",
    "            tmp_img_path_list = images_path_list\n",
    "            images_path_list = []\n",
    "        image_list = [Image.open(image_path).convert('RGB') for image_path in tmp_img_path_list]\n",
    "        image_list = [vis_processors[\"eval\"](image).unsqueeze(0).to(device) for image in image_list]\n",
    "        batch_image = torch.cat(image_list, dim=0)\n",
    "        # first generate the object as a part of the prompt\n",
    "        caption = model.generate({\"image\": batch_image}) #, \"prompt\": [prompt for _ in range(batch_image.shape[0])]\n",
    "        caption_list.extend(caption)\n",
    "  \n",
    "    return caption_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6fdc1c-5742-4b4b-9870-a7d9d01551e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "caption_list = generate_captions(model, vis_processors, list(data['image']))\n",
    "data['caption'] = caption_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f48bc30-4da8-42f8-aae2-1ce5a4fb4c2a",
   "metadata": {},
   "source": [
    "### Extract OCR from Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1ba3b7-8fc9-445a-976c-8ddfb863d3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc915d-371d-4d36-91f9-c19315e6ac6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reader = easyocr.Reader(['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a93c8e-9db3-4d61-8939-fe3e87f09a04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ocr_list=[]\n",
    "for i in tqdm(range(len(data))):\n",
    "    result = reader.readtext(data['image'].iloc[i],detail=0,paragraph=1, batch_size = 8)#, detail = 0\n",
    "    ocr_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45400867-e91b-4259-86c9-95d7a46f3c12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['ocr'] = ocr_list\n",
    "data['ocr'] = data['ocr'].map(lambda x: x[0] if x else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfbd1921-f357-4d32-8fd1-b0ff3317a2a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event</th>\n",
       "      <th>image</th>\n",
       "      <th>text</th>\n",
       "      <th>caption</th>\n",
       "      <th>ocr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020 Summer Olympics</td>\n",
       "      <td>/data/dataset/datakey/images/2014 Hong Kong pr...</td>\n",
       "      <td>Only this side of Lennon Wall left #LennonWallHK</td>\n",
       "      <td>a man and a woman standing in front of a wall ...</td>\n",
       "      <td>Ball nuJnnk CALL;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014 Hong Kong protests</td>\n",
       "      <td>/data/dataset/datakey/images/2020 Summer Olymp...</td>\n",
       "      <td>Jeev Milkha Singh emotionally thanks Neeraj Ch...</td>\n",
       "      <td>a man sitting on top of a bench holding a medal</td>\n",
       "      <td>IVDIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     event                                              image  \\\n",
       "0     2020 Summer Olympics  /data/dataset/datakey/images/2014 Hong Kong pr...   \n",
       "1  2014 Hong Kong protests  /data/dataset/datakey/images/2020 Summer Olymp...   \n",
       "\n",
       "                                                text  \\\n",
       "0   Only this side of Lennon Wall left #LennonWallHK   \n",
       "1  Jeev Milkha Singh emotionally thanks Neeraj Ch...   \n",
       "\n",
       "                                             caption                ocr  \n",
       "0  a man and a woman standing in front of a wall ...  Ball nuJnnk CALL;  \n",
       "1    a man sitting on top of a bench holding a medal              IVDIA  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97db7d7a-fd58-4c97-adae-4eaf9d8dfa22",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Implicit Knowledge Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7715a02-7efa-44ac-899e-af770842bf8c",
   "metadata": {},
   "source": [
    "### 1) Obtaining an API key from OpenAI (Azure https://azure.microsoft.com/en-us/products/ai-services/openai-service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8afc9337-2c3e-45ad-9a60-fb9b100ebfc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da12cd1-bff6-4819-b8db-782135d2003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = 'xx'\n",
    "openai.api_base = 'https://YOUR_RESOURCE_NAME.openai.azure.com/' # your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/\n",
    "openai.api_type = 'azure'\n",
    "openai.api_version = \"2023-08-01-preview\" # this may change in the future"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "889e0c9d-2022-4a73-826e-4955c04665f1",
   "metadata": {},
   "source": [
    "### 2) Formulating an appropriate prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "335419b0-e343-4794-a4e1-b36b6263b673",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Context:Only this side of Lennon Wall left #Le...\n",
       "1    Context:Jeev Milkha Singh emotionally thanks N...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_list=\"Context:\"+data['text']+'\\nCaption:'+data['caption'].iloc[i]+'\\nOCR:'+data['ocr'].iloc[i]+'\\n'+\\\n",
    "\"Question: What's the news event occur?\\nAnswer:\"\n",
    "prompt_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1fc33eb-0845-4046-88cb-ad29c1c906aa",
   "metadata": {},
   "source": [
    "### 3) Extracting generated answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c92f71a9-cb78-4c5f-ab79-151142c2ad82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from aiohttp import ClientSession\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c770fc9f-d19b-4e80-a6c7-627a769b798a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def async_completion(prompt,progress_bar):\n",
    "    progress_bar.update(1)\n",
    "    try:\n",
    "        response = await openai.ChatCompletion.acreate(\n",
    "            engine='xxx',#according to your azure\n",
    "            messages=[\n",
    "                {\"role\": \"assistant\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=0,#the result will be same\n",
    "        )\n",
    "        output=response['choices'][0]['message']['content']\n",
    "    except:\n",
    "        output='Failed' # !!!!api error: should check and retry if contain!!!!!\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a455b24d-9458-4e6b-bdba-a6432e3bcb50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using asyncio for concurrent requests: speed up the api (should lower than the limitation from OpenAI or Azure)\n",
    "SEM_LIMIT = 30  # Number of concurrent requests allowed at the same time\n",
    "RPM_LIMIT = 500  # Requests per minute limit\n",
    "\n",
    "response_list=[]\n",
    "progress_bar = tqdm(total=len(data))\n",
    "sem = asyncio.Semaphore(SEM_LIMIT)\n",
    "async with ClientSession() as session:\n",
    "    openai.aiosession.set(session)\n",
    "    tasks = []\n",
    "    for i in range(0,len(data)):\n",
    "        async with sem:\n",
    "            task = asyncio.ensure_future(async_completion(prompt_list[i], progress_bar))\n",
    "            tasks.append(task)\n",
    "            if len(tasks) % SEM_LIMIT == 0:\n",
    "                response=await asyncio.gather(*tasks)\n",
    "                response_list.extend(response)\n",
    "                tasks = []\n",
    "                await asyncio.sleep(60 / RPM_LIMIT)  # Introducing delays\n",
    "    if tasks:\n",
    "        response = await asyncio.gather(*tasks)\n",
    "        response_list.extend(response)\n",
    "    await openai.aiosession.get().close()\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(asyncio.ensure_future(async_completion(prompt_list[i], progress_bar)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e4f18597-9337-49af-8760-b463e7fa4cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['answer']=response_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7689c57e-4dff-44f5-a775-2869ab302fa1",
   "metadata": {},
   "source": [
    "### 4) Extracting generated explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d0ad0f73-e8fa-4862-abba-d720a6f204db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_list=\"Context:\"+data['text']+'\\nCaption:'+data['caption'].iloc[i]+'\\nOCR:'+data['ocr'].iloc[i]+'\\n'+\\\n",
    "\"Question: What's the news event occur?\\nAnswer:\"+data['answer'].iloc[i]+'\\nThis is because'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177a4c17-6973-45a5-8fa3-64948bb1f4f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_list=[]\n",
    "progress_bar = tqdm(total=len(data))\n",
    "sem = asyncio.Semaphore(SEM_LIMIT)\n",
    "async with ClientSession() as session:\n",
    "    openai.aiosession.set(session)\n",
    "    tasks = []\n",
    "    for i in range(0,len(data)):\n",
    "        async with sem:\n",
    "            task = asyncio.ensure_future(async_completion(prompt_list[i], progress_bar))\n",
    "            tasks.append(task)\n",
    "            if len(tasks) % SEM_LIMIT == 0:\n",
    "                response=await asyncio.gather(*tasks)\n",
    "                response_list.extend(response)\n",
    "                tasks = []\n",
    "                await asyncio.sleep(60 / RPM_LIMIT)  # Introducing delays\n",
    "    if tasks:\n",
    "        response = await asyncio.gather(*tasks)\n",
    "        response_list.extend(response)\n",
    "    await openai.aiosession.get().close()\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(asyncio.ensure_future(async_completion(prompt_list[i], progress_bar)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0c6e7eb0-9fd1-461c-830e-cd64721f8399",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event</th>\n",
       "      <th>image</th>\n",
       "      <th>text</th>\n",
       "      <th>caption</th>\n",
       "      <th>ocr</th>\n",
       "      <th>answer</th>\n",
       "      <th>gpt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020 Summer Olympics</td>\n",
       "      <td>/data/dataset/datakey/images/2014 Hong Kong pr...</td>\n",
       "      <td>Only this side of Lennon Wall left #LennonWallHK</td>\n",
       "      <td>a man and a woman standing in front of a wall ...</td>\n",
       "      <td>Ball nuJnnk CALL;</td>\n",
       "      <td>The news event is about the remaining side of ...</td>\n",
       "      <td>The news event is about the remaining side of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014 Hong Kong protests</td>\n",
       "      <td>/data/dataset/datakey/images/2020 Summer Olymp...</td>\n",
       "      <td>Jeev Milkha Singh emotionally thanks Neeraj Ch...</td>\n",
       "      <td>a man sitting on top of a bench holding a medal</td>\n",
       "      <td>IVDIA</td>\n",
       "      <td>The news event is about Jeev Milkha Singh emot...</td>\n",
       "      <td>The news event is about Jeev Milkha Singh emot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     event                                              image  \\\n",
       "0     2020 Summer Olympics  /data/dataset/datakey/images/2014 Hong Kong pr...   \n",
       "1  2014 Hong Kong protests  /data/dataset/datakey/images/2020 Summer Olymp...   \n",
       "\n",
       "                                                text  \\\n",
       "0   Only this side of Lennon Wall left #LennonWallHK   \n",
       "1  Jeev Milkha Singh emotionally thanks Neeraj Ch...   \n",
       "\n",
       "                                             caption                ocr  \\\n",
       "0  a man and a woman standing in front of a wall ...  Ball nuJnnk CALL;   \n",
       "1    a man sitting on top of a bench holding a medal              IVDIA   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The news event is about the remaining side of ...   \n",
       "1  The news event is about Jeev Milkha Singh emot...   \n",
       "\n",
       "                                                 gpt  \n",
       "0  The news event is about the remaining side of ...  \n",
       "1  The news event is about Jeev Milkha Singh emot...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['gpt']=data['answer']+response_list\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88ef2b21-5b9d-4d81-af59-a32d835c1812",
   "metadata": {},
   "source": [
    "### 5) Utilizing the BERT model for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea1862d-d7b2-4a7c-b54a-92eb8fb46124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20661412-d1ff-4203-8512-05e6a707d528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230cc56c-0e61-4170-ab64-5daba80e1b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_text_features(text_list, max_length=200, batch_size=16):\n",
    "    encoded_inputs = tokenizer(text_list, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    input_ids = encoded_inputs['input_ids'].to(device)\n",
    "    attention_mask = encoded_inputs['attention_mask'].to(device)\n",
    "\n",
    "    # batch\n",
    "    num_samples = len(text_list)\n",
    "    features = []\n",
    "    for i in tqdm(range(0, num_samples, batch_size)):\n",
    "        batch_input_ids = input_ids[i:i+batch_size]\n",
    "        batch_attention_mask = attention_mask[i:i+batch_size]\n",
    "\n",
    "        # extract feature\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "            last_layer_features = outputs.last_hidden_state\n",
    "            pooled_output = torch.mean(last_layer_features, dim=1)\n",
    "\n",
    "        features.extend(pooled_output.cpu().detach().numpy().tolist())\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aff894-d3ba-4b6c-af41-a951a72b9d85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt_feature = extract_text_features(list(data['gpt']))\n",
    "data['gpt_feature']= gpt_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c026a1a-441e-495b-bd74-02776c64ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data\n",
    "data.to_pickle('/data/twitter_gpt.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
